"""
Script Ä‘á»ƒ generate bÃ¡o cÃ¡o káº¿t quáº£ test tá»« Locust CSV files.
Sá»­ dá»¥ng: python generate_report.py [path_to_csv_file]
"""

import sys
import csv
from datetime import datetime
from pathlib import Path


def parse_locust_csv(csv_file_path):
    """
    Parse Locust CSV file vÃ  extract statistics.
    
    Args:
        csv_file_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file CSV tá»« Locust
        
    Returns:
        dict: Dictionary chá»©a statistics
    """
    stats = {
        'endpoints': [],
        'aggregated': {}
    }
    
    try:
        with open(csv_file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                if row['Type'] == 'Aggregated':
                    stats['aggregated'] = {
                        'name': row['Name'],
                        'requests': int(row['Request Count']),
                        'failures': int(row['Failure Count']),
                        'avg_response_time': float(row['Average Response Time']),
                        'min_response_time': float(row['Min Response Time']),
                        'max_response_time': float(row['Max Response Time']),
                        'median_response_time': float(row['Median Response Time']),
                        'p95_response_time': float(row['95%']),
                        'p99_response_time': float(row['99%']),
                        'rps': float(row['Requests/s'])
                    }
                else:
                    endpoint_stats = {
                        'type': row['Type'],
                        'name': row['Name'],
                        'requests': int(row['Request Count']),
                        'failures': int(row['Failure Count']),
                        'avg_response_time': float(row['Average Response Time']),
                        'min_response_time': float(row['Min Response Time']),
                        'max_response_time': float(row['Max Response Time']),
                        'median_response_time': float(row['Median Response Time']),
                        'p95_response_time': float(row['95%']),
                        'p99_response_time': float(row['99%']),
                        'rps': float(row['Requests/s'])
                    }
                    stats['endpoints'].append(endpoint_stats)
    
    except FileNotFoundError:
        print(f"Error: File {csv_file_path} not found!")
        return None
    except Exception as e:
        print(f"Error parsing CSV: {e}")
        return None
    
    return stats


def generate_markdown_report(stats, output_file='../results/result.md'):
    """
    Generate markdown report tá»« statistics.
    
    Args:
        stats: Dictionary chá»©a statistics
        output_file: ÄÆ°á»ng dáº«n file output
    """
    if not stats:
        print("No statistics to generate report!")
        return
    
    report = f"""# ğŸ“Š BÃ¡o CÃ¡o Káº¿t Quáº£ Stress Test - Context Handling Robot API

**NgÃ y test:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Test Tool:** Locust 2.42.6  
**Target Server:** http://103.253.20.30:30020

---

## ğŸ¯ Tá»•ng Quan Test

### Test Configuration
- **Sá»‘ Users:** 10 concurrent users
- **Spawn Rate:** 2 users/second
- **Duration:** 60 giÃ¢y
- **Wait Time:** 1-3 giÃ¢y giá»¯a cÃ¡c requests

### API Endpoints Tested
1. **POST /v1/conversations/end** - Káº¿t thÃºc conversation
2. **POST /v1/activities/suggest** - Gá»£i Ã½ activities cho user

---

## ğŸ“ˆ Káº¿t Quáº£ Test

### Screenshot 1: Statistics Overview
![Test Results Overview](image/result/1764646884789.png)

### Screenshot 2: Detailed Metrics
![Detailed Metrics](image/result/1764646907322.png)

---

## ğŸ“Š PhÃ¢n TÃ­ch Chi Tiáº¿t

### 1. Performance Metrics

"""
    
    # ThÃªm thÃ´ng tin cho tá»«ng endpoint
    for endpoint in stats['endpoints']:
        failure_rate = (endpoint['failures'] / endpoint['requests'] * 100) if endpoint['requests'] > 0 else 0
        
        report += f"""
#### **{endpoint['type']} {endpoint['name']}**
- **Total Requests:** {endpoint['requests']:,}
- **Failures:** {endpoint['failures']} ({failure_rate:.2f}%)
- **Average Response Time:** {endpoint['avg_response_time']:.2f} ms
- **Min Response Time:** {endpoint['min_response_time']:.2f} ms
- **Max Response Time:** {endpoint['max_response_time']:.2f} ms
- **Median (50th percentile):** {endpoint['median_response_time']:.2f} ms
- **95th percentile:** {endpoint['p95_response_time']:.2f} ms
- **99th percentile:** {endpoint['p99_response_time']:.2f} ms
- **Requests per Second (RPS):** {endpoint['rps']:.2f} req/s

"""
    
    # ThÃªm thÃ´ng tin aggregated
    agg = stats['aggregated']
    failure_rate = (agg['failures'] / agg['requests'] * 100) if agg['requests'] > 0 else 0
    
    report += f"""
#### **Aggregated (Tá»•ng Há»£p)**
- **Total Requests:** {agg['requests']:,}
- **Total Failures:** {agg['failures']} ({failure_rate:.2f}%)
- **Average Response Time:** {agg['avg_response_time']:.2f} ms
- **Min Response Time:** {agg['min_response_time']:.2f} ms
- **Max Response Time:** {agg['max_response_time']:.2f} ms
- **Median (50th percentile):** {agg['median_response_time']:.2f} ms
- **95th percentile:** {agg['p95_response_time']:.2f} ms
- **99th percentile:** {agg['p99_response_time']:.2f} ms
- **Total RPS:** {agg['rps']:.2f} req/s

---

## âœ… ÄÃ¡nh GiÃ¡ Káº¿t Quáº£

### Performance Assessment

#### Response Time Analysis
"""
    
    # PhÃ¢n tÃ­ch response time
    avg_time = agg['avg_response_time']
    p95_time = agg['p95_response_time']
    
    if avg_time < 200:
        avg_status = "âœ… Excellent"
    elif avg_time < 500:
        avg_status = "âœ… Good"
    elif avg_time < 1000:
        avg_status = "âš ï¸ Acceptable"
    else:
        avg_status = "âŒ Poor"
    
    if p95_time < 500:
        p95_status = "âœ… Excellent"
    elif p95_time < 1000:
        p95_status = "âœ… Good"
    elif p95_time < 2000:
        p95_status = "âš ï¸ Acceptable"
    else:
        p95_status = "âŒ Poor"
    
    report += f"""
- **Average Response Time:** {avg_time:.2f} ms - {avg_status}
- **95th Percentile:** {p95_time:.2f} ms - {p95_status}

#### Failure Analysis
- **Total Failures:** {agg['failures']}
- **Failure Rate:** {failure_rate:.2f}%
"""
    
    if failure_rate == 0:
        report += "- **Status:** âœ… No failures - Perfect!\n"
    elif failure_rate < 1:
        report += "- **Status:** âœ… Good - Minimal failures\n"
    elif failure_rate < 5:
        report += "- **Status:** âš ï¸ Acceptable - Some failures need investigation\n"
    else:
        report += "- **Status:** âŒ Poor - High failure rate, needs immediate attention\n"
    
    report += f"""
#### Throughput Analysis
- **Average RPS:** {agg['rps']:.2f} req/s
- **RPS Status:** {'âœ… Stable' if agg['rps'] > 0 else 'âŒ No requests'}
- **Total Requests:** {agg['requests']:,} requests

---

## ğŸ¯ Káº¿t Luáº­n

### âœ… Äiá»ƒm Máº¡nh
"""
    
    strengths = []
    if failure_rate == 0:
        strengths.append("KhÃ´ng cÃ³ failures - API hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh")
    if avg_time < 500:
        strengths.append(f"Response time tá»‘t ({avg_time:.2f}ms)")
    if agg['rps'] >= 5:
        strengths.append(f"Throughput tá»‘t ({agg['rps']:.2f} RPS)")
    
    if strengths:
        for i, strength in enumerate(strengths, 1):
            report += f"{i}. {strength}\n"
    else:
        report += "1. Test Ä‘Ã£ hoÃ n thÃ nh\n"
    
    report += """
### âš ï¸ Äiá»ƒm Cáº§n Cáº£i Thiá»‡n
"""
    
    improvements = []
    if failure_rate > 0:
        improvements.append(f"Giáº£m failure rate tá»« {failure_rate:.2f}% xuá»‘ng < 1%")
    if avg_time > 500:
        improvements.append(f"Tá»‘i Æ°u response time (hiá»‡n táº¡i {avg_time:.2f}ms)")
    if p95_time > 1000:
        improvements.append(f"Cáº£i thiá»‡n 95th percentile (hiá»‡n táº¡i {p95_time:.2f}ms)")
    
    if improvements:
        for i, improvement in enumerate(improvements, 1):
            report += f"{i}. {improvement}\n"
    else:
        report += "1. KhÃ´ng cÃ³ Ä‘iá»ƒm nÃ o cáº§n cáº£i thiá»‡n - Káº¿t quáº£ tá»‘t!\n"
    
    report += f"""
### ğŸ“‹ Khuyáº¿n Nghá»‹
1. **Ngáº¯n háº¡n:**
   - {'Kiá»ƒm tra vÃ  fix cÃ¡c failures' if failure_rate > 0 else 'Tiáº¿p tá»¥c monitor performance'}
   - {'Tá»‘i Æ°u response time cho endpoint cháº­m' if avg_time > 500 else 'Maintain current performance'}

2. **DÃ i háº¡n:**
   - Scale up server náº¿u cáº§n xá»­ lÃ½ nhiá»u users hÆ¡n
   - Implement caching Ä‘á»ƒ giáº£m response time
   - Consider load balancing náº¿u RPS tÄƒng cao

---

## ğŸ“Š Performance Summary

| Metric | Value | Status |
|--------|-------|--------|
| Total Requests | {agg['requests']:,} | âœ… |
| Total Failures | {agg['failures']} ({failure_rate:.2f}%) | {'âœ…' if failure_rate < 1 else 'âš ï¸' if failure_rate < 5 else 'âŒ'} |
| Average Response Time | {avg_time:.2f} ms | {'âœ…' if avg_time < 500 else 'âš ï¸' if avg_time < 1000 else 'âŒ'} |
| 95th Percentile | {p95_time:.2f} ms | {'âœ…' if p95_time < 1000 else 'âš ï¸' if p95_time < 2000 else 'âŒ'} |
| 99th Percentile | {agg['p99_response_time']:.2f} ms | {'âœ…' if agg['p99_response_time'] < 2000 else 'âš ï¸'} |
| RPS | {agg['rps']:.2f} req/s | âœ… |

---

## ğŸ” Chi Tiáº¿t Ká»¹ Thuáº­t

### Test Environment
- **Locust Version:** 2.42.6
- **Test Duration:** 60 seconds
- **Concurrent Users:** 10
- **Spawn Rate:** 2 users/second

### Status Codes Accepted
- âœ… **200 OK** - Success
- âœ… **201 Created** - Success
- âœ… **202 Accepted** - Success (Async processing)

---

**Report Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Generated By:** Locust Report Generator Script
"""
    
    # Write to file
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… Report generated successfully: {output_path}")


def main():
    """Main function."""
    if len(sys.argv) < 2:
        print("Usage: python generate_report.py <path_to_csv_file>")
        print("Example: python generate_report.py ../results/stats_20251202_103000.csv")
        return
    
    csv_file = sys.argv[1]
    stats = parse_locust_csv(csv_file)
    
    if stats:
        generate_markdown_report(stats)


if __name__ == '__main__':
    main()

